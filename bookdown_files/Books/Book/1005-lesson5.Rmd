---
editor_options: 
  markdown: 
    wrap: 72
---

# 第五讲：如何清理数据—数据的预处理{#lesson-5}

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse) # Wickham的数据整理的整套工具
pdf.options(height=10/2.54, width=10/2.54, family="GB1") # 注意：此设置要放在最后
```

## **批量读取文件**

我们需要一个干净的数据格式，一般我们会有一个固定的数据格式。如果我们想要进行回归或中介分析等，就需要对数据进行预处理。在本节课中，我们将讲解如何对从问卷或实验软件中下载的数据进行预处理，以得到我们最终想要分析的数据。我们将演示如何将多个数据合并成一个完整的数据，使用for loop或lapply函数。

在使用for loop之前，我们需要了解一些读取文件的知识，包括相对路径等。当计算机或R要与我们本地文件夹交互时，它必须进入读取本地文件夹。如果我们有一个本地文件夹，比如说一个文件夹，那么我们需要找到它的路径，然后将路径输入到命令中，作为一个字符串，输送到处理器中，才能够读取。这就是我们上一节课讲读取CSV文件时所做的事情。我们会给出一个完整的路径或相对路径，让R内部的计算机知道文件在哪里，并对其进行操作，比如read.csv。如果我们要读取一系列文件，我们需要将它们的相对路径读入，然后依次输入到处理器中，让它们被读取并合并成一个完整的数据框。因此，我们需要使用for循环或lapply函数将所有文件的路径列出来，并将它们输入到处理器中，让它们被读取并合并。

我们的输入是文件夹中的文件名。大家可以看一下自己电脑中的文件夹，会发现有很多子文件，比如practice、match和category，它表示的是三个时间的三个阶段 practice表示它在做练习，match表示它在做match的任务，category表示它在做categorization的一个任务。现在我们需要找出所有包含match且以out结尾的文件，因为这些是我们需要读取的文件。

### **通配符**

我们不想逐个列出每个文件名，因为这样太冗长、容易出错且不够简洁。我们可以使用R中的list函数，将文件夹中的所有文件列出来，然后筛选只包含match的文件名。为了实现这一点，我们需要使用通配符，它是一种特殊的符号，可以匹配任意字符。

例如，*.csv代表以csv结尾的所有文件。我们可以使用list函数扫描文件夹，找到所有以match开头且以out结尾的文件名，然后将它们合并成我们需要的文件。我们要做的第一件事是扫描这个文件夹，扫描这个文件夹，把里面所有的文件和文件夹都读取出来。但是这样并不完全符合我们的目标，因为我们只需要符合match这个条件，并且是以out结尾的数据。所以，我们需要列出match文件夹里所有的文件，但这并不符合我们的目标。这时，我们需要使用通配符来匹配文件夹里包含特殊信息的文件。也就是说，我们需要根据文件名是否包含match来筛选文件夹里的文件。

这时，我们需要使用通配符，尤其是问号这个通配符。因为在计算机语言中，问号代表任意数量的任意字符。当我们以 星号.csv结尾时，代表我们只需要扫描以.csv结尾的文件，把它们全部读出来，读取它们的完整文件名。如果它不是以.csv结尾的，我们就跳过它。问号代表单个字符，也就是信号代表任意单个字符。例如，如果我们使用file?，然后是.txt，那么它能够匹配的符合条件的文件就是file1、file2、file3等等。但是如果你是file10，它后面有两个字符，这时它就不匹配了。

中括号里的字符是或的关系，就是说，中括号里的123代表file1后面跟1、2或3都可以。这样可以任意灵活地匹配。因为有可能你不知道文件夹里有多少个文件，你就把它们全部写在中括号里，只要它包含在里面，我们就把它读取出来。

```{r}
# 所有路径使用相对路径
library(here)
# 包含了dplyr和%>%等好用的包的集合
library(tidyverse)
# 养成用相对路径的好习惯，便于其他人运行你的代码
WD <-  here::here()
getwd()
```

### for**循环思路**

那么我们该怎么使用它呢？我们需要先加载tidyverse这个包，提醒大家在开始处理之前要load这个包，否则在使用函数时会出错。这是一个准备工作。接着，我们找到了当前的工作路径。

```{r}
# 把所有符合某种标题的文件全部读取到一个list中
files <- list.files(file.path("data/match"), pattern = "data_exp7_rep_match_.*\\.out$")


head(files, n = 10L)

str(files)
```

我们可以用两种方法来处理这个for循环，其中一种是使用一个变量名files来存储我们从这个文件夹里扫描到的所有文件的名字。在R中，list.files()是一个函数，它可以扫描文件夹里的所有文件。第一个参数是你要在哪个文件夹里扫描。即在当前的工作目录里的data文件夹中，然后在这个文件夹中的match文件夹里进行扫描。我们使用这个pattern来筛选文件夹里的文件名，比如包含了这个match的，或者更加精确的，我们要以这个data_exp7_，然后以repeat开头的文件。然后我们使用一个通配符来表示所有以out结尾的文件都扫描进来。这样扫描完后，我们就得到了所有符合我们这个模式的文件的名字。我们只会得到它们的文件名，不包含它们的路径。如果我们把它们读取出来并列出前面10个，我们就可以看到它们确实完全符合我们上面那个规则。list.files()函数非常有用，因为它可以帮助我们快速地扫描文件夹里的所有文件。

这个操作实际上是扫描一个文件夹，并按照特定模式扫描文件。在R语言中，我们经常使用这个操作来读取数据，这是第一步。我们现在读取到的是一个向量，其中每个元素包含许多字符，每个字符代表一个文件名。这与我们之前讨论的读取单个文件不同。现在我们正在列出一个文件夹中所有符合特定模式或规则的文件名。

读取完后，我们可以使用for循环。首先，我们需要创建一个空的列表来存储读取的数据。for循环的结构是for（条件）{内容}。我们使用i来循环，in表示我们要循环的范围。在for循环中，我们使用i来逐个循环。我们首先将i设置为1，然后做某些事情，然后在i等于2时再做同样的事情，一直到i等于10。

```{r df.mt.out.fl}
# 创建一个空的列表来存储读取的数据框
df_list <- list()
# 循环读取每个文件，处理数据并添加到列表中
for (i in seq_along(files)) { # 重复"读取到的.out个数"的次数
  # 对每个.out，使用read.table
  df <- read.table(file.path("data/match", files[i]), header = TRUE) #read.table似乎比read.csv更聪明，不需要指定分隔符
  # 给读取到的每个.out文件的每个变量统一变量格式
  df <- dplyr::filter(df, Date != "Date") %>% # 因为有些.out文件中部还有变量名，所需需要用filter把这些行过滤掉
    dplyr::mutate(Date = as.character(Date),Prac = as.character(Prac),
                  Sub = as.numeric(Sub),Age = as.numeric(Age),Sex = as.character(Sex),Hand = as.character(Hand),
                  Block = as.numeric(Block),Bin = as.numeric(Bin),Trial = as.numeric(Trial),
                  Shape = as.character(Shape),Label = as.character(Shape),Match = as.character(Match),
                  CorrResp = as.character(CorrResp),Resp = as.character(Resp),
                  ACC = as.numeric(ACC),RT = as.numeric(RT))
  # 将数据框添加到列表中
  df_list[[i]] <- df
}
# 合并所有数据框，只有当变量的属性一致时，才可以bind_rows
# bind_rows 意味着把list中的所有表格整合成一个大表格
df.mt.out.fl <- dplyr::bind_rows(df_list)
# 清除中间变量
rm(df,df_list,files,i)
# 如果你将这个步骤写成函数，则这些变量自然不会出现在全局变量中
```

在for循环中，我们使用df=read.table来读取文件。文件的路径是相对路径，是data match和files的组合。files是文件夹中的一个文件。当i等于1时，我们读取的是第一个文件。我们按照header为true的方式去读取。我们发现out文件里保存的可能不是那么统一，因此我们需要去掉重复的列名。我们可以用一个规则，即如果data再次出现，表示它重复了上面这个列名，我们就把所有这样的行去掉。我们用了上次课选择的那个筛选函数。

反向筛选掉之后，我们还可以对它的变量类型做一些变化。我们基本上就是用了as character和as numerical这两种，把所有的列都变成我们想要的类型。然后，我们生成了一个数据框，并进行了转换。当i等于1时，它就是对第一个文件做了这一番操作。然后，我们把它复制到数据列表里面，第一个位置就是第一个out文件，它经过了转换之后的数据框，就装到数据框列表里面的第一个位置。我们首先将第一个out文件转换成数据框df，并将其放入df列表的第一个位置。然后，我们通过循环读取id位中的第二个文件，并倒数10个，直到读取完所有10个文件。data列表的长度为10，其中包含我们读取的10个数据。

将所有数据放入列表后，我们可以使用bind_rows函数将它们合并成一个大的数据框。bind_rows是Dplyr中一个常用的函数，它可以通过行将不同的数据框合并。因此，data列表实际上是一个完整的列表，其中包含一个一个的out文件，经过初步转换后形成的数据框。由于它们的列都是一模一样的，我们可以直接将它们叠加在一起，最终得到一个完整的数据框。

这就是for循环的逻辑，我们首先读取我们想要的文件名，然后通过迭代的方式一个一个地读取它们，并将它们放入一个DataList中。然后，我们对这个List进行完整的合并。这个思路非常直接，大家都能够理解。当然，理解for循环的前提是我们要理解它的工作原理。为什么我每次讲for循环的时候，助教们可能觉得没有必要呢？因为for循环的使用实际上非常少。但是，我个人认为还是要讲一下for循环。因为for循环不仅可以用在这里，而且可以用在很多场景中。当你需要重复做某一件事情时，没有现成的函数可以帮你完成时，最简单的方法就是写一个for循环，它可以循环迭代帮你完成某一件事情。i可以替代很多东西，比如你有五个东西需要做同样的操作，你可以写一个i从1到5的for循环，将每个东西放进去做同样的操作。这样，你就不用一个一个复制你的代码，然后改变其中的一个值，再运行一遍。如果你经常进行批量处理，学会for循环会非常有帮助。为了更方便地读取并合并文件夹中的文件，我们通常需要使用for循环。

最终的结果应该是25920 obs 16 variables

```{r df.mt.out DT, echo=FALSE}
DT::datatable(head(df.mt.out.fl, 100),
              fillContainer = TRUE, options = list(pageLength = 7))
```

### lapply思路

但是，由于这是一个常见的操作，我们开发了一个更简洁的功能，即使用管道操作符“%>%”来代替for循环。这个符号在DPLYR包中，它将前一步的结果作为下一个函数的输入。我们可以看到，每个符号代表一个操作步骤。我们首先列出符合条件的文件名，并将其作为一个列表输入到lapply函数中。lapply函数是一个apply系列的函数，它将函数应用于一个列表上。在这里，我们将read.table函数应用于列表中的每个元素，其中文件路径是x的参数，head=true是read.table的参数。这个操作实际上就是用一行命令代替了for循环的操作。

```{r df.mt.raw.la}
# 获取所有的.out文件名
df.mt.out.la <- list.files(file.path("data/match"), pattern = "data_exp7_rep_match_.*\\.out$") %>%
  # 对读取到的所有.out文件x都执行函数read.table
  lapply(df.mt.out.la,function(x) read.table(file.path("data/match", x), header = TRUE)) %>% 
  # 对所有被read.table处理过的数据执行dplyr的清洗
  lapply(function(df) dplyr::filter(df, Date != "Date") %>% # 因为有些.out文件中部还有变量名，所需需要用filter把这些行过滤掉
                      dplyr::mutate(Date = as.character(Date),Prac = as.character(Prac),
                                    Sub = as.numeric(Sub),Age = as.numeric(Age),Sex = as.character(Sex),Hand = as.character(Hand),
                                    Block = as.numeric(Block),Bin = as.numeric(Bin),Trial = as.numeric(Trial),
                                    Shape = as.character(Shape),Label = as.character(Shape),Match = as.character(Match),
                                    CorrResp = as.character(CorrResp),Resp = as.character(Resp),
                                    ACC = as.numeric(ACC),RT = as.numeric(RT)
                                    ) # 有些文件里读出来的数据格式不同，在这里统一所有out文件中的数据格式
         ) %>%
  bind_rows()
```

最终，我们得到一个包含所有数据框的列表，这些数据框都经过了一系列的转换和操作，然后使用“bind_rows”将它们合并成一个数据框。虽然这个过程看起来很冗长，但思路清晰。最终，我们得到一个包含16个变量和25000多个观测值的数据框。在读取数据后，我们通常会保存中间结果，以便下次使用。我们可以再次运行这个过程，但其实重新做一遍也无妨，因为数据基本上不会变。你只需要每次运行上面的代码，然后直接使用write.csv将读取的数据保存下来。write.csv很简单，只需要将csv文件写下来即可。我们前面整理并合并后的数据框需要一个名字和路径。我们可以使用相对路径，在当前工作目录下的data/match文件夹中将其保存为match-row.csv。一个常用的参数是强制将行名写入文件中，即row names等于false。

```{r write.csv}
#for loop 或 lapply的都可以
#write.csv(df.mt.out.fl, file = "./data/match/match_raw.csv",row.names = FALSE)
write.csv(df.mt.out.la, file = "./data/match/match_raw.csv",row.names = FALSE)
```

## 数据预处理

假设我们已经准备好了match和penguin的数据，我们可以读取数据并开始今天的数据预处理。我们使用刚才保存的csv文件，使用head等于true和separator等于逗号来处理数据。

在tidyverse中，filter和mutate是常用的功能之一。groupby可以根据某些变量对数据进行分组，但一定要记得使用ungroup。使用summarize可以计算均值、标准差、标准误等统计量。将groupby和summarize结合起来，我们可以快速有效地得到心理学中常用的统计量，如均值、标准差、标准误和计分数等。我们可以对数据进行分组和条件筛选。刚才提到的“ungroup”是指在“summarize”之后取消分组。而“select”函数是用来选择列，与“filter”函数选择行不同。有时我们也可以用“select”函数重新排序，而“arrange”函数则是用来对整个数据框按某一列的值进行排序。

接下来我们来看一个例子，假设我们要选择1995年或之后出生的人，我们可以在管道中使用“filter”函数。管道操作有两种方式，一种是直接从输入开始，另一种是用管道符号连接函数。在“dplyr”中，函数的参数是有顺序的，第一个是数据框。我们可以用点来代表输入数据，然后用“age”作为筛选条件。筛选完后，我们可以把结果输入到下一个函数中。如果要保留结果，我们需要把它复制到一个新的变量中。比如，我们可以选择与肃清障碍相关的问卷。

```{r example of select rawdata_penguin}
# 使用select选择age和ALEX的所有题目
df.clean.select <- df.pg.raw %>%
  dplyr::select(age, starts_with("ALEX"), eatdrink, avoidance)
#笨一点的方法，就是把16个ALEX都写出来
```
```{r example of select rawdata_penguin DT, echo=FALSE}
# 看看其他变量是不是都消失了
DT::datatable(head(df.clean.select, 10),
              fillContainer = TRUE, options = list(pageLength = 3))
```

startwith是tidyverse中的一个包，可以方便地选择以“Alex”开头的所有列。它本质上是一个简化的通配符，因为在DPIY中，我们经常需要选择以某个特定开头或结尾的列，如果每次都写通配符会很麻烦。使用startwith包可以直接选择以“Alex”开头的列，并得到一串字符，可以和其他选择一起使用。

使用mutate函数可以生成一个新的变量，不仅仅是求和，还可以进行任意转换，比如加减乘除或判断。在这里，我们使用mutate函数将前四个“Alex”列的得分求和，得到一个新的变量“Alex3”，表示前四个“Alex”列的得分总和。

```{r example of mutate_1 rawdata_penguin}
# 把ALEX1 - 4求和
df.clean.mutate_1 <- df.pg.raw %>% 
  dplyr::mutate(ALEX_SUM = ALEX1 + ALEX2 + ALEX3 + ALEX4)
```
```{r example of mutate_1 rawdata_penguin DT, echo=FALSE}
# 看看是不是真的求和了
DT::datatable(head(df.clean.mutate_1, 10),
              fillContainer = TRUE, options = list(pageLength = 3))
```

需要注意的是，它是逐行运算，可以使用其他函数如rowSums，其中也用到了通配符。我们可以利用DPRY中的筛选功能，选取所有以Alex为开头的列，并对这些列进行逐行求和，以得到真正反映Alex所有项目的总和。这种方法将给出16个项目的总和，而前面提到的方法只有4个。

```{r example of mutate_2 rawdata_penguin}
# 对所有含有ALEX的列求和
df.clean.mutate_2 <- df.pg.raw %>% 
  dplyr::mutate(ALEX_SUM = rowSums(select(., starts_with("ALEX"))))
```
```{r example of mutate_2 rawdata_penguin DT, echo=FALSE}
DT::datatable(head(df.clean.mutate_2, 10),
              fillContainer = TRUE, options = list(pageLength = 3))
```

此外，我们还可以使用mutate函数对数据进行重新编码，例如根据出生年龄将其分成不同的年龄段。我们可以使用case_when函数生成一个新变量，该变量根据条件变为不同的值。这种方法可以用于反向编码，例如将原来等于1的值变为5，将原来等于2的值变为4。这些技巧在心理学中非常常见，而case_when函数则是解决这些问题的好方法。

```{r example of mutate_3 rawdata_penguin}
df.clean.mutate_3 <- df.pg.raw %>% 
  dplyr::mutate(decade = case_when(age <= 1969 ~ 60,
                                   age >= 1970 & age <= 1979 ~ 70,
                                   age >= 1980 & age <= 1989 ~ 80,
                                   age >= 1990 & age <= 1999 ~ 90,
                                   TRUE ~ NA_real_)
                ) %>% #当括号多的时候注意括号的位置 
  dplyr::select(.,decade, everything())
```
```{r example of mutate_3 rawdata_penguin DT, echo=FALSE}
DT::datatable(head(df.clean.mutate_3, 10),
              fillContainer = TRUE, options = list(pageLength = 3))
```

我们可以先按照出生年代将数据进行分组，然后使用group by函数重新分组。我们还可以按照多个条件进行分组，例如按照年代和性别进行分组。分组完成后，我们可以使用summarize函数对每个组内部进行操作，例如求均值和标准差等。最后，我们可以使用ungroup函数将数据重新拆分，以便进行后续的运算。

```{r example of group_by rawdata_penguin}
df.clean.group_by <- df.clean.mutate_3 %>%
  dplyr::group_by(.,decade) %>% # 根据被试的出生年代，将数据拆分
  dplyr::summarise(mean_avoidance = mean(avoidance)) %>% # 计算不同年代下被试的平均avoidance
  dplyr::ungroup()
```
```{r example of group_by rawdata_penguin DT, echo=FALSE}
# 拆分文件并不会让源文件产生任何视觉上的变化
DT::datatable(head(df.clean.group_by, 4),
              fillContainer = TRUE, options = list(pageLength = 4))
```

我们可以将所有学到的函数串起来，例如1.先使用filter函数选择eat drink为1的base，然后2.使用select函数选择所需变量，3.再使用mutate函数对出生年份进行编码，4.最后使用groupby和summarize函数求出按照年代求alex的均值。5.最后，我们可以使用row sum函数对alex求均值。

```{r example of total rawdata_penguin}
df.pg.clean <- df.pg.raw %>%
  dplyr::filter(eatdrink == 1) %>% # 选择eatdrink为1的被试
  dplyr::select(age, starts_with("ALEX"), eatdrink, avoidance) %>%
  dplyr::mutate(ALEX1 = case_when(ALEX1 == '1' ~ '5', # 反向计分
                                  ALEX1 == '2' ~ '4',
                                  ALEX1 == '3' ~ '3',
                                  ALEX1 == '4' ~ '2',
                                  ALEX1 == '5' ~ '1',
                                  TRUE ~ as.character(ALEX1))) %>%
  dplyr::mutate(ALEX1 = as.numeric(ALEX1)) %>%
  dplyr::mutate(ALEX_SUM = rowSums(select(., starts_with("ALEX"))), # 把所有ALEX的题目分数求和
                decade = case_when(age <= 1969 ~ 60, # 把出生年份转换为年代
                                   age >= 1970 & age <= 1979 ~ 70,
                                   age >= 1980 & age <= 1989 ~ 80,
                                   age >= 1990 & age <= 1999 ~ 90,
                                   TRUE ~ NA_real_)) %>%
  dplyr::group_by(decade) %>% # 按照年代将数据拆分
  dplyr::summarise(mean_ALEX = mean(ALEX_SUM)) %>% # 计算每个年代的被试的平均的ALEX_SUM
  dplyr::ungroup() # 解除对数据的拆分
```

在这里，我们可以按照多个条件进行分组，但我们只使用了一个条件，即数十年，因此可以写得很简洁。我们使用summarize函数计算alex总分的平均值，然后取消分组。如果我们对alex这个变量感兴趣，我们可以得到每个数十年在alex得分上的平均值。

在这个过程中，我们需要理解管道的操作方式，因为它可以处理我们想要对某个变量的所有领域进行的所有操作。每个函数都可以单独使用，但将它们放入管道中可以省略一些步骤。Tidyverse中有许多包，例如startwith、case1和rowsum，它们都是Dplyr和Tidyverse系统中的函数。另一个常用的包是tidyr，它可以使我们的数据更加整洁。在Tidyverse中，我们可以使用一些常用的函数，比如separate、extract、unite和pivot。其中，separate可以按照特定规则将一个变量分割成为几列，比如将日期按照年月日分成三列。extract可以提取一个或多个特定的字符串。unite则是将多个列合并成为一列。pivot则可以将宽格式数据转换为长格式数据或者将长格式数据转换为宽格式数据。

**长数据和宽数据**

那么，什么是长格式数据和宽格式数据呢？在SPSS中，我们通常使用长格式数据来记录实验数据，比如eprime数据。长格式数据的形式可能是这样的：每个实验对象有一个ID，然后有不同的条件和变量，比如年龄和性别。每个实验对象可能有多个条件，因此我们需要以行来记录数据。每一行代表一个观察值。例如，如果我们有一个问卷调查，其中有五个问题，那么我们将每个问题的回答记录在一列中，每一行代表一个受访者。相反，宽格式数据是指我们将变量记录在多列中，每一行代表一个观察值。例如，如果我们有一个实验，其中有五个条件，那么我们将每个条件的结果记录在一列中，每一行代表一个受试者。

> 长数据和宽数据是数据分析和处理中常用的两种不同的数据结构。
>
> 长数据通常表示为一列数据包含多个变量，每个变量在不同的行中重复出现。例如，一列包含所有参与者的年龄、性别和教育水平，每个参与者在数据集中有多行记录。长数据集通常适用于需要进行聚合和统计分析的情况。
>
> 宽数据则通常表示为多列数据包含多个变量，每个变量在同一行中出现。例如，一列包含参与者的年龄，另一列包含性别，第三列包含教育水平。每个参与者只有一行记录。宽数据集通常适用于需要进行分组和比较分析的情况。
>
> 两种数据结构都有其优缺点，具体使用哪种结构取决于数据的分析目的和方法。
>
> 当一个数据集中的每一行都是单个观察单位时，通常使用宽数据格式。下面是一些宽数据格式的例子：
>
> - 一份包含人口统计数据的电子表格，每一行表示一个城市或地区，每一列则包含不同的人口统计数据，如总人口数、平均年龄、平均收入等等。
> - 一个电商网站的订单数据集，每一行表示一个订单，每一列包含订单的属性，如订单号、购买日期、购买者姓名、商品名称、数量、价格等等。
> - 一个医疗研究的数据集，每一行表示一个受试者，每一列包含该受试者的不同测量结果，如身高、体重、血压、血糖等等。
>
> 当数据集中的每一行表示的是一个观察单位的不同取值时，通常使用长数据格式。下面是一些长数据格式的例子：
>
> - 一份学生考试成绩的数据集，每一行表示一个学生的一门考试成绩，每一列包含学生的属性（如学生ID、姓名、年级、学科等）和考试成绩。
> - 一个心理学实验的数据集，每一行表示一个受试者在实验中的一次操作，每一列包含操作的属性（如受试者ID、实验条件、操作类型等）和操作结果。
> - 一份股票市场的数据集，每一行表示某支股票在某个时间点的市场数据，每一列包含市场数据的属性（如股票代码、日期、开盘价、收盘价、成交量等）。

最后，我们需要注意数据中的缺失值。有时候，我们需要将缺失值删除，以便我们可以更好地分析数据。我们可以使用dropNA函数来删除缺失值，但是我们需要谨慎使用它，因为它可能会导致一些问题。

在Match数据集中，我们有一个名为Shape的变量，它代表了社会意义。这个变量实际上包含了两个分类变量：Valence和Identity。Valence代表了情感的极性，而Identity代表了所属类别。我们需要将这两个变量分开处理，以便我们可以更好地分析数据。我们需要将数据分成两个部分，一个是自我，一个是他人。可以使用分隔符将它们分开。在这里，我们使用分隔符来代表上面的数据点。Code等于Shape，这意味着我们要对哪个列进行分割。我们将一个列分成两个，分别命名为Valence和Identity。我们使用一种比较高级的方式来进行分割，实际上是将其分成两部分，一部分是道德或不道德，另一部分是自我或他人。这种方式不是唯一的，我们可以想出其他方式来进行分割。我们需要将原来的道德、自我等分割成一行，一个列是Valence，另一个列是Identity，对应的是自我或他人。我们需要按照何种规则进行分割，这里显示的是一种规则。有时候，数据本身就带有分割符号，如短横线、下划线或点等。如果数据内在带有这种符号，使用这种符号进行分割更好。对于这个Shape列进行分割后，我们可以选择重新排序列，将感兴趣的列放到前面，使用Select函数进行操作。如果分割正确，我们才能进行列的选择和排序。在这个例子中，我们可以看到Base、Valence和Identity出现了分割的效果。以前的第一行Date和实验类型等全部移到了后面。这是我们对分隔符功能的第一个讲解。第二步是提取数据，与第一步相似。但是这一步是通过字符匹配的方式提取数据，我们使用正则表达式进行匹配，然后提取Shape字段为两个列，一个叫Valence，一个叫Identity。在提取后，我们并不会删除原来的Shape列，而是将其保留。这样做可以得到与第一步相同的结果。然而，在处理字符时可能会遇到困难，因为我们需要按照特定的方式进行匹配。为了解决这个问题，我们可以使用正则表达式（RegEx）来帮助我们进行匹配。在Tidyverse中，可以找到RegEx的Cheat Sheet，其中包含了各种符号的含义，这样我们就可以更方便地编写正则表达式了。我们可以在Tidyverse中搜索StringR的Cheat Sheet，其中包含了很多常见的符号及其含义。当然，我们也可以使用Unite函数将分开的两列重新组合起来。在TidyR中，我们可以使用Unite函数将两列合并，并为新列命名。Sep参数表示用什么分隔符来分割不同列中的字符串。例如，我们可以使用两个冒号来表示空格。我们需要将RT和ACC转换成一个名为DV的变量。我们将原来的subject、RT和ACC三个列转换成一个subject01、DV和value三个列。其中，DV列包含了RT和ACC的值，value列则分别对应着RT和ACC的值。我们使用PivotLonger函数将宽格式的数据转换成长格式的数据。在转换过程中，我们需要对原来的列进行索引，以便在新的长格式数据中保存原来的信息。为此，我们需要使用NamesTo参数来指定新的列名。在进行PivotWider操作时，我们需要将原来的数值变成新的列，也就是将数据从长格式转换为宽格式。为了进行逆向操作，我们需要了解每个索引的独特性。为了简化操作，我们先对RT和ACC求均值，然后按照subject、trial和dv进行分组。其中，dv是原来的列名，现在变成了新的列名value。我们通过groupBy对均值进行转换，然后从列中得到新的宽格式数据的列名。我们有两个列，一个是shape，一个是dv。我们用这两个列来计算均值。我们需要从多个列中得到新的列名，我们用下划线将两个列组合起来。最终得到的数据是每个条件下的均值，比如正确率和反应时间。这是一个常见的长宽转换操作。如果你使用R来处理数据，删除缺失值是一个常见的操作。你可以使用dropna函数，它可以帮助你删除包含缺失值的行，从而得到每个列都没有缺失值的数据。你可以使用paste或print函数打印出结果，以便检查你是否正确地删除了行。接下来，你可以选择你需要的变量，删除缺失值，选择符合标准的倍数，计算平均反应时间和正确率，进行数据的长宽转换，使用mutate函数计算音变量效率。选择变量可以使用select函数，然后使用DFMT clean函数得到一个结果，再使用右手法则的贝叶斯排除无效应答，选择合理的反应时间，计算每个贝叶斯在每个实验条件下的均值。你可以使用sub和shape函数来计算平均正确率和反应时间，然后使用ungroup函数取消分组。最后，你可以使用mutate函数生成一个新的变量效率，它是反应时间除以正确率的结果。你可以使用shape函数对变量进行拆分，以便在画图时更方便。我们可以有以下选择：首先筛选符合match条件的数据，然后根据moral条件选择valence，最终得到我们想要的结果。接着，我们将其进行长宽转换，变成自我和他人条件下的数据。选择base、identity和efficiency进行转换，然后将其变成宽的数据，其中identity为names，efficiency为values。最终得到每一个base在每一种条件下，在match条件下自我和他人的efficiency。在计算SPE时，我们需要计算自我参照自我优先加工效应，即想要知道自我和他人在加工上是否有差异。为了计算我们感兴趣的效应量，我们需要将其转换成宽的数据。现在是提问时间，如果有哪个地方不太懂，可以先练习一下前面的内容。我们可以先练习计算信号检测论的D'，然后再尝试运行第47页的代码。大家可以在自己的电脑上运行代码，保证能够得到和我们一样的结果，然后再看每一行代码的含义。建议先将数据读取进来，然后将完整的代码复制到自己的脚本文件中，选择前面的代码进行运行。如果有不懂的地方，可以向小组长寻求帮助。可以对录音稿进行简化和润色，使其更加易读易懂。首先，我们可以删减代码中的某些部分，以便更好地理解代码的运行过程。删减代码的方法是在代码前面加上注释。这样，我们就可以选择一部分代码并运行它，以便更好地了解每一行代码的作用。此外，我们还可以使用注释来检查代码，以便更好地测试不同的方法。最后，我们需要记住两个小目标：首先，我们需要将数据读入程序中；其次，我们需要逐行运行代码并随时提出问题。如果有任何不理解的地方，请随时举手。我们回到之前提到的读取原始数据的部分。我们需要将多个out文件合并成一个原始数据，并保存为df.mt.raw。同样地，我们也需要将tanguindata读取为df.pg.raw。现在，我们需要完成一个课外作业，即将所有match数据合并成一个完整的实验数据集，并计算d'信号检测论。我们已经有了代码，可以使用Lplot读取包含match数据的文件，并将其组合成一个单一的数据集。我们还可以将其写成一个csv文件。因此，你的任务就是操作这个数据集，计算d'信号检测论。信号检测论是以match作为信号，以mismatch作为噪音。我们需要计算击中率、虚报率、错失率和正确拒绝率。我们已经给出了计算模板，但是希望你能够写出完整的工作流程，从原始数据开始，进行一系列的筛选，最终得到每个倍式的d'值。由于我们有两个条件，即valence和identity，因此你需要给出一个简洁的结果，包括每个倍式的每个条件的d'值。我们需要的数据包含四个字段，分别是受试者编号、情感价值、编号和d'值。每个数据点都有自己的一行，形成一个宽表格数据。如果您有任何问题，请随时在小组长的群里讨论。下周我们将让大家分享自己的想法，因此请大家做好准备。
